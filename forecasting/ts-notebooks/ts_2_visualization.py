# -*- coding: utf-8 -*-
"""ts-2-visualization.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cA1eLLE-_Hp2IkiZzLF1kpDnMsM5iX_8

# Time series manipulation and visualization

In the last notebook, we saw how to fetch metrics from a prometheus instance using the prometheus api client. In this notebook, we will learn how to manipulate and visualize the fetched time series data.
* As an example, we take "node_memory_Active_bytes" which are the memory bytes that have been recently used by a node. It reflects the memory pressure on the node.
* In the following cells, we show how to perform basic operations on this time series and then how to descriptively visualize them in graphs.
"""

from prometheus_api_client import PrometheusConnect
from prometheus_api_client.metric_range_df import MetricRangeDataFrame
from prometheus_api_client.utils import parse_datetime
from datetime import timedelta
import pandas as pd
from IPython.display import Image
import plotly.graph_objects as go

"""## Fetch the metric data
* We extract "node_memory_Active_bytes" for the last 7 days.
"""

# Creating the prometheus connect object with the required parameter
prom_url = "http://demo.robustperception.io:9090"
pc = PrometheusConnect(url=prom_url, disable_ssl=True)

# Request last week's data
metric_data = pc.get_metric_range_data(
    "node_memory_Active_bytes",  # metric name and label config
    start_time=parse_datetime(
        "7 days ago"
    ),  # datetime object for metric range start time
    end_time=parse_datetime(
        "now"
    ),  # datetime object for metric range end time
    chunk_size=timedelta(
        days=1
    ),  # timedelta object for duration of metric data downloaded in one request
)

## Make the dataframe
metric_df = MetricRangeDataFrame(metric_data)
metric_df.index = pd.to_datetime(metric_df.index, unit="s", utc=True)

"""## Visualization
* For visualization, we are going to use plotly interactive plots.
* The next graph shows active bytes in the node for the week.
* We can interatively zoom in and zoom out the graph. Try selecting a time frame to zoom in.
"""

fig = go.Figure()
fig.add_trace(go.Scatter(x=metric_df.index, y=metric_df["value"]))
fig.update_layout(
    title="Node memory active bytes", xaxis_title="Time", yaxis_title="Bytes"
)

"""* Sometimes the data is huge, and since interactive plots show each point in the graph, it can take a while to load.
* To solve that problem you can try two things: plot static graph or downsample (both explained next).

### Static plot
"""

## The way to generate static images
img_bytes = fig.to_image(format="png")
Image(img_bytes)

"""## Data Manipulation

### Indexing
* Here we see how to get data for a particular period of time.
"""

# Get data for today
today = str(parse_datetime("now").date())
print("Date today: ", today)
metric_df_td = metric_df.loc[today]
metric_df_td
# You can also do metric_df.loc['2020'] to get all values for the year 2020

fig = go.Figure(go.Scatter(x=metric_df_td.index, y=metric_df_td["value"]))
fig.update_layout(title="Indexing", xaxis_title="Time", yaxis_title="Bytes")

"""### Slicing 
* Here we see how to get data for a range of time.
"""

# Get data for today but only from 6 to 9 PM
# Note the index strings created for slicing
metric_df_slice = metric_df.loc[today + " 18:00" : today + " 21:00"]
metric_df_slice

fig = go.Figure(
    go.Scatter(x=metric_df_slice.index, y=metric_df_slice["value"])
)
fig.update_layout(title="Slicing", xaxis_title="Time", yaxis_title="Bytes")
fig.show()

"""* Try getting this same snapshot in the first image by selecting this slice in the interactive plot.

### Resampling
* Resampling is used to get the data in a different time frequency.
* It is very important because certain patterns become apparent only when you resample. For example, you may not observe seasonality in hourly CPU usage data but it may be visible in the weekly data. 

#### Downsampling
* Here we reduce the frequency of sampling. For example, daily data to weekly data.
* The reduction needs an aggregation method.
* Therefore, we have to specify how to reduce: mean, sum, max, etc.
* For specifying frequencies we can use frequency [aliases](https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#timeseries-offset-aliases) like 'D' for day and 'H' for hour.
"""

## Get a pandas time series by type conversion of object to float
ts = metric_df["value"].astype(float)

## Downsample to daily frequency
ts_d = ts.resample("D").mean()
ts_d

## Downsample to 12 hour frequency
ts_h = ts.resample("12H").mean()
ts_h

## Downsample to 30 min frequency
ts_30_min = ts.resample("30min").mean()
ts_30_min

## Plot the downsamples
fig = go.Figure()
fig.add_trace(go.Scatter(x=ts.index, y=ts, name="Original"))
fig.add_trace(go.Scatter(x=ts_30_min.index, y=ts_30_min, name="Every 30 mins"))
fig.add_trace(go.Scatter(x=ts_d.index, y=ts_d, name="Daily"))
fig.update_layout(
    title="Resampling",
    xaxis_title="Time",
    yaxis_title="Bytes",
    legend_title="Frequency",
)
fig.show()

"""### Upsampling
* Upsampling is possible but new information is added to the data in this step since we are converting from low resolution to high resolution, for e.g., weekly data to daily data.
* Upsampling is less common but is used usually when analyzing multiple time series that should have the same frequency of observation.
* The next example shows upsampling with forward fill algorithm (taking the previous valid value for filling missing one).
"""

## Downsample to hourly data and then upsample to min frequency
ts_uh = ts.resample("H").mean().resample("min").ffill()
fig = go.Figure()
fig.add_trace(go.Scatter(x=ts_h.index, y=ts_h, name="Hourly"))
fig.add_trace(go.Scatter(x=ts_uh.index, y=ts_uh, name="Upsampled minutely"))
fig.update_layout(
    title="Uplsampling",
    xaxis_title="Time",
    yaxis_title="Bytes",
    legend_title="Frequency",
)
fig.show()

"""### Rolling
* Rolling is used to aggreate along a window around a data point.
* A good use of rolling is smoothing the time series.
* For example, here we look at 60 min, and 360 min data window averages of node memory.
"""

ts_min = ts.resample("min").mean()
ts_rm = ts_min.rolling(60, center=True).mean()
ts_rm_ext = ts_min.rolling(360, center=True).mean()

fig = go.Figure()
fig.add_trace(go.Scatter(x=ts_min.index, y=ts_min, name="Original"))
fig.add_trace(go.Scatter(x=ts_rm.index, y=ts_rm, name="60 min mean"))
fig.add_trace(go.Scatter(x=ts_rm_ext.index, y=ts_rm_ext, name="360 min mean"))
fig.update_layout(
    title="Rolling",
    xaxis_title="Time",
    yaxis_title="Bytes",
    legend_title="Rolling mean",
)
fig.show()

"""### Standard deviation
* For the times when you don't trust mean, here's how to plot the series with standard deviation
"""

ts_min = ts.resample("min").mean()
ts_rm_ext = ts_min.rolling(360, center=True)
ts_rm_mean = ts_rm_ext.mean()
ts_rm_std = ts_rm_ext.std().fillna(0)


fig = go.Figure()
fig.add_trace(
    go.Scatter(
        x=ts_rm_mean.index,
        y=ts_rm_mean,
        error_y=dict(
            type="data",
            array=ts_rm_std.values,
            color="lightblue",
            thickness=0.5,
            visible=True,
        ),
    )
)
fig.update_layout(
    title="Standard deviation",
    xaxis_title="Time",
    yaxis_title="Bytes",
)
fig.show()

"""### Conclusion
* With that, you know the basics of dealing with and plotting time series data. We will be using a lot of these methods over the coming notebooks. Next, we are going into delve into statistics with time series data.

### References

Thanks to the shoulders we stand on.

* https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html
* https://tomaugspurger.github.io/modern-7-timeseries
* https://jakevdp.github.io/PythonDataScienceHandbook/03.11-working-with-time-series.html
"""

